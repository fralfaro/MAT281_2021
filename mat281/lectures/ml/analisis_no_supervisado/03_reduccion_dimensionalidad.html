
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Reducci√≥n de dimensionalidad &#8212; Aplicaciones de la Matematica en la Ingenieria</title>
    
  <link href="../../../../_static/css/theme.css" rel="stylesheet" />
  <link href="../../../../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../../../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../../../../_static/sphinx-book-theme.acff12b8f9c144ce68a297486a2fa670.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../../../../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="../../../../" src="../../../../_static/documentation_options.js"></script>
    <script src="../../../../_static/jquery.js"></script>
    <script src="../../../../_static/underscore.js"></script>
    <script src="../../../../_static/doctools.js"></script>
    <script src="../../../../_static/togglebutton.js"></script>
    <script src="../../../../_static/clipboard.min.js"></script>
    <script src="../../../../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../../../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../../../../_static/sphinx-thebe.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <link rel="shortcut icon" href="../../../../_static/logo_python.jpeg"/>
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" />
    <link rel="next" title="Overfitting" href="../overfitting/04_overfitting_underfitting.html" />
    <link rel="prev" title="Clustering" href="03_clustering.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../../../index.html">
      
      <img src="../../../../_static/logo_python.jpeg" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Aplicaciones de la Matematica en la Ingenieria</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../index.html">
   Home
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Herramientas B√°sicas
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../basic_tools/lecture_000_intro.html">
   Introducci√≥n
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../basic_tools/lecture_000_configuraciones.html">
     Configuraciones
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../basic_tools/lecture_011_os.html">
   Linux
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../basic_tools/lecture_015_git.html">
   Git
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../basic_tools/lecture_013_ide.html">
   IDE‚Äôs
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../basic_tools/lecture_016_venv.html">
   Virtual Environment
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../basic_tools/lecture_012_python.html">
   Python
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../basic_tools/lecture_014_jupyter.html">
   Jupyter
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Computaci√≥n cientifica
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../data_manipulation/scientific_computing/numpy.html">
   Numpy
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../data_manipulation/scientific_computing/scipy.html">
   SciPy
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../data_manipulation/scientific_computing/sympy.html">
   Sympy
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Manipulaci√≥n de datos
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../data_manipulation/data_manipulation/base_datos.html">
   Base de datos
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../data_manipulation/data_manipulation/pandas/pandas.html">
   Pandas
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../data_manipulation/data_manipulation/modulos_pandas/groupby.html">
     Groupby
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../data_manipulation/data_manipulation/modulos_pandas/merge_concat.html">
     Merge &amp; Concat
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../data_manipulation/data_manipulation/modulos_pandas/pivot.html">
     Pivot
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../data_manipulation/data_manipulation/sqlalchemy/sqlalchemy.html">
   SQLAlchemy
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Visualizaci√≥n
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../data_manipulation/visualization/visualizacion/introduction.html">
   Introducci√≥n
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../data_manipulation/visualization/visualizacion/imperativa.html">
   Visualizaci√≥n Imperativa
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../data_manipulation/visualization/visualizacion/matplotlib.html">
     Matplotlib
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../data_manipulation/visualization/visualizacion/declarativa.html">
   Visualizaci√≥n Declarativa
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../data_manipulation/visualization/visualizacion/seaborn.html">
     Seaborn
    </a>
   </li>
  </ul>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  An√°lisis exploratorio de datos
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../data_manipulation/eda/intro.html">
   Introducci√≥n
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../data_manipulation/eda/eda.html">
   Caso Aplicado
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Machine Learning
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../introduccion_ml/introduccion.html">
   Introducci√≥n
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../analisis_supervisado_regresion/intro.html">
   Aprendizaje supervisado - Regresi√≥n
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../analisis_supervisado_regresion/02_regresion_lineal.html">
     Modelos de regressi√≥n lineal
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../analisis_supervisado_regresion/02_modelos_regresion.html">
     Modelos de regressi√≥n multiple
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../analisis_supervisado_clasificacion/intro.html">
   Aprendizaje supervisado - Clasificaci√≥n
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../analisis_supervisado_clasificacion/02_clasificacion.html">
     Modelo de regresi√≥n log√≠stica
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="../introduccion_ml/no_supervisados.html">
   Aprendizaje no supervisado
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
  <label for="toctree-checkbox-7">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="03_clustering.html">
     Clustering
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     Reducci√≥n de dimensionalidad
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../overfitting/04_overfitting_underfitting.html">
   Overfitting
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
  <label for="toctree-checkbox-8">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../overfitting/04_reducir_overfitting.html">
     Reducir el overfitting
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../time_series/05_series_temporales.html">
   Series Temporales
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../../../../_sources/mat281/lectures/ml/analisis_no_supervisado/03_reduccion_dimensionalidad.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://gitlab.com/FAAM/mat281_2021"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#introduccion">
   Introducci√≥n
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#algoritmos-de-reduccion-de-la-dimensionalidad">
   Algoritmos de reducci√≥n de la dimensionalidad
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#metodos-de-algebra-lineal">
     M√©todos de √°lgebra lineal
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#multiples-metodos-de-aprendizaje">
     M√∫ltiples m√©todos de aprendizaje
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#pca">
   PCA
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#interpretacion-geometrica-de-las-componentes-principales">
     Interpretaci√≥n geom√©trica de las componentes principales
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#calculo-de-las-componentes-principales">
     C√°lculo de las componentes principales
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#caracteristicas-del-pca">
     Caracter√≠sticas del PCA
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#proporcion-de-varianza-explicada">
     Proporci√≥n de varianza explicada
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#numero-optimo-de-componentes-principales">
     N√∫mero √≥ptimo de componentes principales
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#aplicacion">
     Aplicaci√≥n
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#t-distributed-stochastic-neighbor-embedding-t-sne">
   t-Distributed Stochastic Neighbor Embedding (t-SNE)
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#comparando-con-pca">
     Comparando con PCA
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#explicacion">
     Explicaci√≥n
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id1">
     Aplicaci√≥n
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#otros-metodos-de-reduccion-de-dimensionalidad">
   Otros m√©todos de reducci√≥n de dimensionalidad
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id2">
     Aplicaci√≥n
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#referencia">
   Referencia
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="reduccion-de-dimensionalidad">
<h1>Reducci√≥n de dimensionalidad<a class="headerlink" href="#reduccion-de-dimensionalidad" title="Permalink to this headline">¬∂</a></h1>
<div class="section" id="introduccion">
<h2>Introducci√≥n<a class="headerlink" href="#introduccion" title="Permalink to this headline">¬∂</a></h2>
<p>En aprendizaje autom√°tico y estad√≠sticas <a class="reference external" href="https://es.wikipedia.org/wiki/Reducci%C3%B3n_de_dimensionalidad">reducci√≥n de dimensionalidad</a>  es el proceso de reducci√≥n del n√∫mero de variables aleatorias que se trate, y se puede dividir en selecci√≥n de funci√≥n y extracci√≥n de funci√≥n</p>
<p>Sin embargo, se puede utilizar como un paso de preprocesamiento de transformaci√≥n de datos para algoritmos de aprendizaje autom√°tico en conjuntos de datos de modelado predictivo de clasificaci√≥n y regresi√≥n con algoritmos de aprendizaje supervisado.</p>
<p>Hay muchos algoritmos de reducci√≥n de dimensionalidad entre los que elegir y no existe el mejor algoritmo para todos los casos. En cambio, es una buena idea explorar una variedad de algoritmos de reducci√≥n de dimensionalidad y diferentes configuraciones para cada algoritmo.</p>
</div>
<div class="section" id="algoritmos-de-reduccion-de-la-dimensionalidad">
<h2>Algoritmos de reducci√≥n de la dimensionalidad<a class="headerlink" href="#algoritmos-de-reduccion-de-la-dimensionalidad" title="Permalink to this headline">¬∂</a></h2>
<p>Hay muchos algoritmos que pueden ser usados para la reducci√≥n de la dimensionalidad.</p>
<p>Dos clases principales de m√©todos son los que se extraen del √°lgebra lineal y los que se extraen del aprendizaje m√∫ltiple.</p>
<div class="section" id="metodos-de-algebra-lineal">
<h3>M√©todos de √°lgebra lineal<a class="headerlink" href="#metodos-de-algebra-lineal" title="Permalink to this headline">¬∂</a></h3>
<p>Los m√©todos de factorizaci√≥n matricial extra√≠dos del campo del √°lgebra lineal pueden utilizarse para la dimensionalidad. Algunos de los m√©todos m√°s populares incluyen:</p>
<ul class="simple">
<li><p>An√°lisis de los componentes principales</p></li>
<li><p>Descomposici√≥n del valor singular</p></li>
<li><p>Factorizaci√≥n de matriz no negativa</p></li>
</ul>
</div>
<div class="section" id="multiples-metodos-de-aprendizaje">
<h3>M√∫ltiples m√©todos de aprendizaje<a class="headerlink" href="#multiples-metodos-de-aprendizaje" title="Permalink to this headline">¬∂</a></h3>
<p>Los m√∫ltiples m√©todos de aprendizaje buscan una proyecci√≥n de dimensiones inferiores de alta entrada dimensional que capte las propiedades salientes de los datos de entrada.</p>
<p>Algunos de los m√©todos m√°s populares incluyen:</p>
<ul class="simple">
<li><p>Isomap Embedding</p></li>
<li><p>Locally Linear Embedding</p></li>
<li><p>Multidimensional Scaling</p></li>
<li><p>Spectral Embedding</p></li>
<li><p>t-distributed Stochastic Neighbor Embedding (t-sne)</p></li>
</ul>
<p>Cada algoritmo ofrece un enfoque diferente para el desaf√≠o de descubrir las relaciones naturales en los datos de dimensiones inferiores.</p>
<p>No hay un mejor algoritmo de reducci√≥n de la dimensionalidad, y no hay una manera f√°cil de encontrar el mejor algoritmo para sus datos sin usar experimentos controlados.</p>
<p>Debido a la importancia que se tiene en el mundo del machine lerning, se dar√° un explicaci√≥n formal del m√©todo de PCA y luego se dar√° una breve rese√±a de los dem√°s m√©todos.</p>
</div>
</div>
<div class="section" id="pca">
<h2>PCA<a class="headerlink" href="#pca" title="Permalink to this headline">¬∂</a></h2>
<p>El an√°lisis de componentes principales (Principal Component Analysis PCA) es un m√©todo de reducci√≥n de dimensionalidad que permite simplificar la complejidad de espacios con m√∫ltiples dimensiones a la vez que conserva su informaci√≥n.</p>
<p>Sup√≥ngase que existe una muestra con  <span class="math notranslate nohighlight">\(n\)</span>  individuos cada uno con  <span class="math notranslate nohighlight">\(p\)</span>  variables ( <span class="math notranslate nohighlight">\(X_1\)</span>,‚Ä¶,<span class="math notranslate nohighlight">\(X_p\)</span>), es decir, el espacio muestral tiene  <span class="math notranslate nohighlight">\(p\)</span>  dimensiones. PCA permite encontrar un n√∫mero de factores subyacentes  (<span class="math notranslate nohighlight">\(z&lt;p\)</span>)  que explican aproximadamente lo mismo que las  <span class="math notranslate nohighlight">\(p\)</span>  variables originales. Donde antes se necesitaban  <span class="math notranslate nohighlight">\(p\)</span>  valores para caracterizar a cada individuo, ahora bastan  <span class="math notranslate nohighlight">\(z\)</span>  valores. Cada una de estas  <span class="math notranslate nohighlight">\(z\)</span>  nuevas variables recibe el nombre de componente principal.</p>
<a class="reference internal image-reference" href="../../../../_images/pca_01.png"><img alt="../../../../_images/pca_01.png" class="align-center" src="../../../../_images/pca_01.png" style="width: 600px; height: 480px;" /></a>
<p>El m√©todo de PCA permite por lo tanto ‚Äúcondensar‚Äù la informaci√≥n aportada por m√∫ltiples variables en solo unas pocas componentes. Aun as√≠, no hay que olvidar que sigue siendo necesario disponer del valor de las variables originales para calcular las componentes. Dos de las principales aplicaciones del PCA son la visualizaci√≥n y el preprocesado de predictores previo ajuste de modelos supervisados.</p>
<div class="section" id="interpretacion-geometrica-de-las-componentes-principales">
<h3>Interpretaci√≥n geom√©trica de las componentes principales<a class="headerlink" href="#interpretacion-geometrica-de-las-componentes-principales" title="Permalink to this headline">¬∂</a></h3>
<p>Una forma intuitiva de entender el proceso de PCA es interpretar las componentes principales desde un punto de vista geom√©trico. Sup√≥ngase un conjunto de observaciones para las que se dispone de dos variables ( <span class="math notranslate nohighlight">\(X_1\)</span>, <span class="math notranslate nohighlight">\(X_2\)</span> ). El vector que define la primera componente principal (<span class="math notranslate nohighlight">\(Z_1\)</span> ) sigue la direcci√≥n en la que las observaciones tienen m√°s varianza (l√≠nea roja). La proyecci√≥n de cada observaci√≥n sobre esa direcci√≥n equivale al valor de la primera componente para dicha observaci√≥n (principal component score,  <span class="math notranslate nohighlight">\(z_{i1}\)</span> ).</p>
<a class="reference internal image-reference" href="../../../../_images/pca_02.png"><img alt="../../../../_images/pca_02.png" class="align-center" src="../../../../_images/pca_02.png" style="width: 600px; height: 480px;" /></a>
<p>La segunda componente ( <span class="math notranslate nohighlight">\(Z_2\)</span> ) sigue la segunda direcci√≥n en la que los datos muestran mayor varianza y que no est√° correlacionada con la primera componente. La condici√≥n de no correlaci√≥n entre componentes principales equivale a decir que sus direcciones son perpendiculares/ortogonales.</p>
<a class="reference internal image-reference" href="../../../../_images/pca_03.png"><img alt="../../../../_images/pca_03.png" class="align-center" src="../../../../_images/pca_03.png" style="width: 600px; height: 480px;" /></a>
</div>
<div class="section" id="calculo-de-las-componentes-principales">
<h3>C√°lculo de las componentes principales<a class="headerlink" href="#calculo-de-las-componentes-principales" title="Permalink to this headline">¬∂</a></h3>
<p>Cada componente principal ( <span class="math notranslate nohighlight">\(Z_i\)</span> ) se obtiene por combinaci√≥n lineal de las variables originales. Se pueden entender como nuevas variables obtenidas al combinar de una determinada forma las variables originales. La primera componente principal de un grupo de variables ( <span class="math notranslate nohighlight">\(X_1,...,X_p\)</span> ) es la combinaci√≥n lineal normalizada de dichas variables que tiene mayor varianza:</p>
<div class="math notranslate nohighlight">
\[ Z_1 = \phi_{11}X_1 + ... + \phi_{p1}X_p\]</div>
<p>Que la combinaci√≥n lineal sea normalizada implica que:</p>
<div class="math notranslate nohighlight">
\[\sum_{j=1}^p \phi^2_{j1} = 1\]</div>
<p>Los t√©rminos  <span class="math notranslate nohighlight">\(\phi_{11},...,\phi_{p1}\)</span>  reciben en el nombre de loadings y son los que definen las componentes. Por ejemplo,  <span class="math notranslate nohighlight">\(\phi_{11}\)</span>  es el loading de la variable  <span class="math notranslate nohighlight">\(X_1\)</span>  de la primera componente principal. Los loadings pueden interpretarse como el peso/importancia que tiene cada variable en cada componente y, por lo tanto, ayudan a conocer que tipo de informaci√≥n recoge cada una de las componentes.</p>
<p>Dado un set de datos  <span class="math notranslate nohighlight">\(X\)</span>  con <span class="math notranslate nohighlight">\(n\)</span> observaciones y <span class="math notranslate nohighlight">\(p\)</span> variables, el proceso a seguir para calcular la primera componente principal es:</p>
<ul class="simple">
<li><p>Centrar las variables: se resta a cada valor la media de la variable a la que pertenece. Con esto se consigue que todas las variables tengan media cero.</p></li>
<li><p>Se resuelve un problema de optimizaci√≥n para encontrar el valor de los loadings con los que se maximiza la varianza. Una forma de resolver esta optimizaci√≥n es mediante el c√°lculo de eigenvector-eigenvalue de la matriz de covarianzas.</p></li>
</ul>
<p>Una vez calculada la primera componente ( <span class="math notranslate nohighlight">\(Z_1\)</span> ), se calcula la segunda ( <span class="math notranslate nohighlight">\(Z_2\)</span> ) repitiendo el mismo proceso pero a√±adiendo la condici√≥n de que la combinaci√≥n lineal no pude estar correlacionada con la primera componente. Esto equivale a decir que  <span class="math notranslate nohighlight">\(Z_1\)</span>  y  <span class="math notranslate nohighlight">\(Z_2\)</span>  tienen que ser perpendiculares. EL proceso se repite de forma iterativa hasta calcular todas las posibles componentes (<em>min(<span class="math notranslate nohighlight">\(n-1, p\)</span>)</em>) o hasta que se decida detener el proceso. El orden de importancia de las componentes viene dado por la magnitud del eigenvalue asociado a cada eigenvector.</p>
</div>
<div class="section" id="caracteristicas-del-pca">
<h3>Caracter√≠sticas del PCA<a class="headerlink" href="#caracteristicas-del-pca" title="Permalink to this headline">¬∂</a></h3>
<ul class="simple">
<li><p><strong>Escalado de las variables</strong>: El proceso de PCA identifica las direcciones con mayor varianza.</p></li>
<li><p><strong>Reproducibilidad de las componentes</strong>: El proceso de PCA est√°ndar es determinista, genera siempre las mismas componentes principales, es decir, el valor de los loadings resultantes es el mismo.</p></li>
<li><p><strong>Influencia de outliers</strong>: Al trabajar con varianzas, el m√©todo PCA es muy sensible a outliers, por lo que es recomendable estudiar si los hay. La detecci√≥n de valores at√≠picos con respecto a una determinada dimensi√≥n es algo relativamente sencillo de hacer mediante comprobaciones gr√°ficas.</p></li>
</ul>
</div>
<div class="section" id="proporcion-de-varianza-explicada">
<h3>Proporci√≥n de varianza explicada<a class="headerlink" href="#proporcion-de-varianza-explicada" title="Permalink to this headline">¬∂</a></h3>
<p>Una de las preguntas m√°s frecuentes que surge tras realizar un PCA es: ¬øCu√°nta informaci√≥n presente en el set de datos original se pierde al proyectar las observaciones en un espacio de menor dimensi√≥n? o lo que es lo mismo ¬øCuanta informaci√≥n es capaz de capturar cada una de las componentes principales obtenidas? Para contestar a estas preguntas se recurre a la proporci√≥n de varianza explicada por cada componente principal.</p>
<p>Asumiendo que las variables se han normalizado para tener media cero, la varianza total presente en el set de datos se define como</p>
<div class="math notranslate nohighlight">
\[\sum_{j=1}^p Var(X_j) = \dfrac{1}{n}\sum_{j=1}^p\sum_{i=1}^nx_{ij}^2\]</div>
<p>y la varianza explicada por la componente m es</p>
<div class="math notranslate nohighlight">
\[\dfrac{1}{n}\sum_{i=1}^n z_{im}^2 = \dfrac{1}{n}\sum_{i=1}^n (\sum_{j=1}^p \phi_{jm}x_{ij})^2\]</div>
<p>Por lo tanto, la proporci√≥n de varianza explicada por la componente m viene dada por el ratio</p>
<div class="math notranslate nohighlight">
\[ \dfrac{\sum_{i=1}^n (\sum_{j=1}^p \phi_{jm}x_{ij})^2}{\sum_{j=1}^p\sum_{i=1}^nx_{ij}^2}\]</div>
<p>Tanto la proporci√≥n de varianza explicada, como la proporci√≥n de varianza explicada acumulada, son dos valores de gran utilidad a la hora de decidir el n√∫mero de componentes principales a utilizar en los an√°lisis posteriores. Si se calculan todas las componentes principales de un set de datos, entonces, aunque transformada, se est√° almacenando toda la informaci√≥n presente en los datos originales. El sumatorio de la proporci√≥n de varianza explicada acumulada de todas las componentes es siempre 1.</p>
</div>
<div class="section" id="numero-optimo-de-componentes-principales">
<h3>N√∫mero √≥ptimo de componentes principales<a class="headerlink" href="#numero-optimo-de-componentes-principales" title="Permalink to this headline">¬∂</a></h3>
<p>Por lo general, dada una matriz de datos de dimensiones <span class="math notranslate nohighlight">\(n \times p\)</span>, el n√∫mero de componentes principales que se pueden calcular es como m√°ximo de <span class="math notranslate nohighlight">\(n-1\)</span> o <span class="math notranslate nohighlight">\(p\)</span> (el menor de los dos valores es el limitante). Sin embargo, siendo el objetivo del PCA reducir la dimensionalidad, suelen ser de inter√©s utilizar el n√∫mero m√≠nimo de componentes que resultan suficientes para explicar los datos. No existe una respuesta o m√©todo √∫nico que permita identificar cual es el n√∫mero √≥ptimo de componentes principales a utilizar. Una forma de proceder muy extendida consiste en evaluar la proporci√≥n de varianza explicada acumulada y seleccionar el n√∫mero de componentes m√≠nimo a partir del cual el incremento deja de ser sustancial.</p>
<a class="reference internal image-reference" href="../../../../_images/pca_04.png"><img alt="../../../../_images/pca_04.png" class="align-center" src="../../../../_images/pca_04.png" style="width: 600px; height: 480px;" /></a>
</div>
<div class="section" id="aplicacion">
<h3>Aplicaci√≥n<a class="headerlink" href="#aplicacion" title="Permalink to this headline">¬∂</a></h3>
<p>El m√©todo Principal Components Regression PCR consiste en ajustar un modelo de regresi√≥n lineal por m√≠nimos cuadrados empleando como predictores las componentes generadas a partir de un Principal Component Analysis (PCA). De esta forma, con un n√∫mero reducido de componentes se puede explicar la mayor parte de la varianza de los datos.</p>
<p>En los estudios observacionales, es frecuente disponer de un n√∫mero elevado de variables que se pueden emplear como predictores, sin embargo, esto no implica necesariamente que se disponga de mucha informaci√≥n. Si las variables est√°n correlacionadas entre ellas, la informaci√≥n que aportan es redundante y adem√°s, se incumple la condici√≥n de no colinealidad necesaria en la regresi√≥n por m√≠nimos cuadrados. Dado que el PCA es √∫til eliminando informaci√≥n redundante, si se emplean como predictores las componentes principales, se puede mejorar el modelo de regresi√≥n. Es importante tener en cuenta que, si bien el Principal Components Regression reduce el n√∫mero de predictores del modelo, no se puede considerar como un m√©todo de selecci√≥n de variables ya que todas ellas se necesitan para el c√°lculo de las componentes. La identificaci√≥n del n√∫mero √≥ptimo de componentes principales que se emplean como predictores en PCR puede identificarse por validaci√≥n cruzada.</p>
<blockquote>
<div><p><strong>Datos</strong>: El set de datos <code class="docutils literal notranslate"><span class="pre">USArrests</span></code> contiene el porcentaje de asaltos (Assault), asesinatos (Murder) y secuestros (Rape) por cada 100,000 habitantes para cada uno de los 50 estados de USA (1973). Adem√°s, tambi√©n incluye el porcentaje de la poblaci√≥n de cada estado que vive en zonas rurales (UrbanPoP).</p>
</div></blockquote>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># librerias</span>
 
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span> 


<span class="n">pd</span><span class="o">.</span><span class="n">set_option</span><span class="p">(</span><span class="s1">&#39;display.max_columns&#39;</span><span class="p">,</span> <span class="mi">500</span><span class="p">)</span>  <span class="c1"># Ver m√°s columnas de los dataframes</span>

<span class="c1"># Ver gr√°ficos de matplotlib en jupyter notebook/lab</span>
<span class="o">%</span><span class="k">matplotlib</span> inline
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">cross_val_score</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">RepeatedStratifiedKFold</span>
<span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">Pipeline</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_squared_error</span>
<span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">make_pipeline</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">scale</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Datos</span>
<span class="n">datos</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;data/USArrests.csv&#39;</span><span class="p">)</span>
<span class="n">datos</span> <span class="o">=</span> <span class="n">datos</span><span class="o">.</span><span class="n">rename</span><span class="p">(</span><span class="n">columns</span> <span class="o">=</span> <span class="p">{</span><span class="n">datos</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span><span class="s1">&#39;index&#39;</span><span class="p">})</span><span class="o">.</span><span class="n">set_index</span><span class="p">(</span><span class="s1">&#39;index&#39;</span><span class="p">)</span>
<span class="n">datos</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Murder</th>
      <th>Assault</th>
      <th>UrbanPop</th>
      <th>Rape</th>
    </tr>
    <tr>
      <th>index</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Alabama</th>
      <td>13.2</td>
      <td>236</td>
      <td>58</td>
      <td>21.2</td>
    </tr>
    <tr>
      <th>Alaska</th>
      <td>10.0</td>
      <td>263</td>
      <td>48</td>
      <td>44.5</td>
    </tr>
    <tr>
      <th>Arizona</th>
      <td>8.1</td>
      <td>294</td>
      <td>80</td>
      <td>31.0</td>
    </tr>
    <tr>
      <th>Arkansas</th>
      <td>8.8</td>
      <td>190</td>
      <td>50</td>
      <td>19.5</td>
    </tr>
    <tr>
      <th>California</th>
      <td>9.0</td>
      <td>276</td>
      <td>91</td>
      <td>40.6</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Veamos una exploraci√≥n inicial de los datos:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;----------------------&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Media de cada variable&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;----------------------&#39;</span><span class="p">)</span>
<span class="n">datos</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>----------------------
Media de cada variable
----------------------
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Murder        7.788
Assault     170.760
UrbanPop     65.540
Rape         21.232
dtype: float64
</pre></div>
</div>
</div>
</div>
<p>La media de las variables muestra que hay tres veces m√°s secuestros que asesinatos y 8 veces m√°s asaltos que secuestros.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;-------------------------&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Varianza de cada variable&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;-------------------------&#39;</span><span class="p">)</span>
<span class="n">datos</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>-------------------------
Varianza de cada variable
-------------------------
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Murder        18.970465
Assault     6945.165714
UrbanPop     209.518776
Rape          87.729159
dtype: float64
</pre></div>
</div>
</div>
</div>
<p>La varianza es muy distinta entre las variables, en el caso de Assault, la varianza es varios √≥rdenes de magnitud superior al resto.</p>
<p>Si no se estandarizan las variables para que tengan media cero y desviaci√≥n est√°ndar de uno antes de realizar el estudio PCA, la variable Assault, que tiene una media y dispersi√≥n muy superior al resto, dominar√° la mayor√≠a de las componentes principales.</p>
<p><strong>Modelo PCA</strong></p>
<p>La clase <code class="docutils literal notranslate"><span class="pre">sklearn.decomposition.PCA</span></code> incorpora las principales funcionalidades que se necesitan a la hora de trabajar con modelos PCA. El argumento <code class="docutils literal notranslate"><span class="pre">n_components</span></code> determina el n√∫mero de componentes calculados. Si se indica None, se calculan todas las posibles (min(filas, columnas) - 1).</p>
<p>Por defecto, <code class="docutils literal notranslate"><span class="pre">PCA()</span></code> centra los valores pero no los escala. Esto es importante ya que, si las variables tienen distinta dispersi√≥n, como en este caso, es necesario escalarlas. Una forma de hacerlo es combinar un <code class="docutils literal notranslate"><span class="pre">StandardScaler()</span></code> y un <code class="docutils literal notranslate"><span class="pre">PCA()</span></code> dentro de un <code class="docutils literal notranslate"><span class="pre">pipeline</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Entrenamiento modelo PCA con escalado de los datos</span>
<span class="c1"># ==============================================================================</span>
<span class="n">pca_pipe</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span><span class="n">StandardScaler</span><span class="p">(),</span> <span class="n">PCA</span><span class="p">())</span>
<span class="n">pca_pipe</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">datos</span><span class="p">)</span>

<span class="c1"># Se extrae el modelo entrenado del pipeline</span>
<span class="n">modelo_pca</span> <span class="o">=</span> <span class="n">pca_pipe</span><span class="o">.</span><span class="n">named_steps</span><span class="p">[</span><span class="s1">&#39;pca&#39;</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<p>Una vez entrenado el objeto <code class="docutils literal notranslate"><span class="pre">PCA</span></code>, pude accederse a toda la informaci√≥n de las componentes creadas.</p>
<p><code class="docutils literal notranslate"><span class="pre">components_</span></code> contiene el valor de los loadings  ùúô  que definen cada componente (eigenvector). Las filas se corresponden con las componentes principals (ordenadas de mayor a menor varianza explicada). Las filas se corresponden con las variables de entrada.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Se combierte el array a dataframe para a√±adir nombres a los ejes.</span>
<span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
    <span class="n">data</span>    <span class="o">=</span> <span class="n">modelo_pca</span><span class="o">.</span><span class="n">components_</span><span class="p">,</span>
    <span class="n">columns</span> <span class="o">=</span> <span class="n">datos</span><span class="o">.</span><span class="n">columns</span><span class="p">,</span>
    <span class="n">index</span>   <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;PC1&#39;</span><span class="p">,</span> <span class="s1">&#39;PC2&#39;</span><span class="p">,</span> <span class="s1">&#39;PC3&#39;</span><span class="p">,</span> <span class="s1">&#39;PC4&#39;</span><span class="p">]</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Murder</th>
      <th>Assault</th>
      <th>UrbanPop</th>
      <th>Rape</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>PC1</th>
      <td>0.535899</td>
      <td>0.583184</td>
      <td>0.278191</td>
      <td>0.543432</td>
    </tr>
    <tr>
      <th>PC2</th>
      <td>0.418181</td>
      <td>0.187986</td>
      <td>-0.872806</td>
      <td>-0.167319</td>
    </tr>
    <tr>
      <th>PC3</th>
      <td>-0.341233</td>
      <td>-0.268148</td>
      <td>-0.378016</td>
      <td>0.817778</td>
    </tr>
    <tr>
      <th>PC4</th>
      <td>0.649228</td>
      <td>-0.743407</td>
      <td>0.133878</td>
      <td>0.089024</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Analizar con detalle el vector de loadings que forma cada componente puede ayudar a interpretar qu√© tipo de informaci√≥n recoge cada una de ellas. Por ejemplo, la primera componente es el resultado de la siguiente combinaci√≥n lineal de las variables originales:</p>
<div class="math notranslate nohighlight">
\[PC1=0.535899 Murder+0.583184 Assault+0.278191 UrbanPop+0.543432 Rape\]</div>
<p>Los pesos asignados en la primera componente a las variables Assault, Murder y Rape son aproximadamente iguales entre ellos y superiores al asignado a UrbanPoP. Esto significa que la primera componente recoge mayoritariamente la informaci√≥n correspondiente a los delitos. En la segunda componente, es la variable UrbanPoP es la que tiene con diferencia mayor peso, por lo que se corresponde principalmente con el nivel de urbanizaci√≥n del estado. Si bien en este ejemplo la interpretaci√≥n de las componentes es bastante clara, no en todos los casos ocurre lo mismo, sobre todo a medida que aumenta el n√∫mero de variables.</p>
<p>La influencia de las variables en cada componente analizarse visualmente con un gr√°fico de tipo heatmap.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Heatmap componentes</span>
<span class="c1"># ==============================================================================</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
<span class="n">componentes</span> <span class="o">=</span> <span class="n">modelo_pca</span><span class="o">.</span><span class="n">components_</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">componentes</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;viridis&#39;</span><span class="p">,</span> <span class="n">aspect</span><span class="o">=</span><span class="s1">&#39;auto&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">datos</span><span class="o">.</span><span class="n">columns</span><span class="p">)),</span> <span class="n">datos</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">datos</span><span class="o">.</span><span class="n">columns</span><span class="p">)),</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">modelo_pca</span><span class="o">.</span><span class="n">n_components_</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">();</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../../_images/03_reduccion_dimensionalidad_23_0.png" src="../../../../_images/03_reduccion_dimensionalidad_23_0.png" />
</div>
</div>
<p>Una vez calculadas las componentes principales, se puede conocer la varianza explicada por cada una de ellas, la proporci√≥n respecto al total y la proporci√≥n de varianza acumulada. Esta informaci√≥n est√° almacenada en los atributos <code class="docutils literal notranslate"><span class="pre">explained_variance_</span></code> y <code class="docutils literal notranslate"><span class="pre">explained_variance_ratio_</span></code> del modelo.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># graficar varianza por componente</span>
<span class="n">percent_variance</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">modelo_pca</span><span class="o">.</span><span class="n">explained_variance_ratio_</span><span class="o">*</span> <span class="mi">100</span><span class="p">,</span> <span class="n">decimals</span> <span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;PC1&#39;</span><span class="p">,</span> <span class="s1">&#39;PC2&#39;</span><span class="p">,</span> <span class="s1">&#39;PC3&#39;</span><span class="p">,</span> <span class="s1">&#39;PC4&#39;</span><span class="p">]</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">x</span><span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">5</span><span class="p">),</span> <span class="n">height</span><span class="o">=</span><span class="n">percent_variance</span><span class="p">,</span> <span class="n">tick_label</span><span class="o">=</span><span class="n">columns</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">modelo_pca</span><span class="o">.</span><span class="n">n_components_</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Componente principal&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Por. varianza explicada&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Porcentaje de varianza explicada por cada componente&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../../_images/03_reduccion_dimensionalidad_25_0.png" src="../../../../_images/03_reduccion_dimensionalidad_25_0.png" />
</div>
</div>
<p>Ahora realizamos el gr√°fico pero respecto a la suma acumulada.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># graficar varianza por la suma acumulada de los componente</span>
<span class="n">percent_variance_cum</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">percent_variance</span><span class="p">)</span>
<span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;PC1&#39;</span><span class="p">,</span> <span class="s1">&#39;PC1+PC2&#39;</span><span class="p">,</span> <span class="s1">&#39;PC1+PC2+PC3&#39;</span><span class="p">,</span> <span class="s1">&#39;PC1+PC2+PC3+PC4&#39;</span><span class="p">]</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">x</span><span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">5</span><span class="p">),</span> <span class="n">height</span><span class="o">=</span><span class="n">percent_variance_cum</span><span class="p">,</span> <span class="n">tick_label</span><span class="o">=</span><span class="n">columns</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Percentate of Variance Explained&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Principal Component Cumsum&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;PCA Scree Plot&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../../_images/03_reduccion_dimensionalidad_27_0.png" src="../../../../_images/03_reduccion_dimensionalidad_27_0.png" />
</div>
</div>
<p>Si se empleasen √∫nicamente las dos primeras componentes se conseguir√≠a explicar el 87% de la varianza observada.</p>
<p><strong>Transformaci√≥n</strong></p>
<p>Una vez entrenado el modelo, con el m√©todo <code class="docutils literal notranslate"><span class="pre">transform()</span></code> se puede reducir la dimensionalidad de nuevas observaciones proyect√°ndolas en el espacio definido por las componentes.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Proyecci√≥n de las observaciones de entrenamiento</span>
<span class="c1"># ==============================================================================</span>
<span class="n">proyecciones</span> <span class="o">=</span> <span class="n">pca_pipe</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X</span><span class="o">=</span><span class="n">datos</span><span class="p">)</span>
<span class="n">proyecciones</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
    <span class="n">proyecciones</span><span class="p">,</span>
    <span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;PC1&#39;</span><span class="p">,</span> <span class="s1">&#39;PC2&#39;</span><span class="p">,</span> <span class="s1">&#39;PC3&#39;</span><span class="p">,</span> <span class="s1">&#39;PC4&#39;</span><span class="p">],</span>
    <span class="n">index</span>   <span class="o">=</span> <span class="n">datos</span><span class="o">.</span><span class="n">index</span>
<span class="p">)</span>
<span class="n">proyecciones</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>PC1</th>
      <th>PC2</th>
      <th>PC3</th>
      <th>PC4</th>
    </tr>
    <tr>
      <th>index</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Alabama</th>
      <td>0.985566</td>
      <td>1.133392</td>
      <td>-0.444269</td>
      <td>0.156267</td>
    </tr>
    <tr>
      <th>Alaska</th>
      <td>1.950138</td>
      <td>1.073213</td>
      <td>2.040003</td>
      <td>-0.438583</td>
    </tr>
    <tr>
      <th>Arizona</th>
      <td>1.763164</td>
      <td>-0.745957</td>
      <td>0.054781</td>
      <td>-0.834653</td>
    </tr>
    <tr>
      <th>Arkansas</th>
      <td>-0.141420</td>
      <td>1.119797</td>
      <td>0.114574</td>
      <td>-0.182811</td>
    </tr>
    <tr>
      <th>California</th>
      <td>2.523980</td>
      <td>-1.542934</td>
      <td>0.598557</td>
      <td>-0.341996</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>La transformaci√≥n es el resultado de multiplicar los vectores que definen cada componente con el valor de las variables. Puede calcularse de forma manual:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">proyecciones</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">modelo_pca</span><span class="o">.</span><span class="n">components_</span><span class="p">,</span> <span class="n">scale</span><span class="p">(</span><span class="n">datos</span><span class="p">)</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
<span class="n">proyecciones</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">proyecciones</span><span class="p">,</span> <span class="n">index</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;PC1&#39;</span><span class="p">,</span> <span class="s1">&#39;PC2&#39;</span><span class="p">,</span> <span class="s1">&#39;PC3&#39;</span><span class="p">,</span> <span class="s1">&#39;PC4&#39;</span><span class="p">])</span>
<span class="n">proyecciones</span> <span class="o">=</span> <span class="n">proyecciones</span><span class="o">.</span><span class="n">transpose</span><span class="p">()</span><span class="o">.</span><span class="n">set_index</span><span class="p">(</span><span class="n">datos</span><span class="o">.</span><span class="n">index</span><span class="p">)</span>
<span class="n">proyecciones</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>PC1</th>
      <th>PC2</th>
      <th>PC3</th>
      <th>PC4</th>
    </tr>
    <tr>
      <th>index</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Alabama</th>
      <td>0.985566</td>
      <td>1.133392</td>
      <td>-0.444269</td>
      <td>0.156267</td>
    </tr>
    <tr>
      <th>Alaska</th>
      <td>1.950138</td>
      <td>1.073213</td>
      <td>2.040003</td>
      <td>-0.438583</td>
    </tr>
    <tr>
      <th>Arizona</th>
      <td>1.763164</td>
      <td>-0.745957</td>
      <td>0.054781</td>
      <td>-0.834653</td>
    </tr>
    <tr>
      <th>Arkansas</th>
      <td>-0.141420</td>
      <td>1.119797</td>
      <td>0.114574</td>
      <td>-0.182811</td>
    </tr>
    <tr>
      <th>California</th>
      <td>2.523980</td>
      <td>-1.542934</td>
      <td>0.598557</td>
      <td>-0.341996</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p><strong>Reconstrucci√≥n</strong></p>
<p>Puede revertirse la transformaci√≥n y reconstruir el valor inicial con el m√©todo inverse_transform(). Es importante tener en cuenta que, la reconstrucci√≥n, solo ser√° completa si se han incluido todas las componentes.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Recostruccion de las proyecciones</span>
<span class="c1"># ==============================================================================</span>
<span class="n">recostruccion</span> <span class="o">=</span> <span class="n">pca_pipe</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="n">X</span><span class="o">=</span><span class="n">proyecciones</span><span class="p">)</span>
<span class="n">recostruccion</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
                    <span class="n">recostruccion</span><span class="p">,</span>
                    <span class="n">columns</span> <span class="o">=</span> <span class="n">datos</span><span class="o">.</span><span class="n">columns</span><span class="p">,</span>
                    <span class="n">index</span>   <span class="o">=</span> <span class="n">datos</span><span class="o">.</span><span class="n">index</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;------------------&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Valores originales&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;------------------&#39;</span><span class="p">)</span>
<span class="n">display</span><span class="p">(</span><span class="n">recostruccion</span><span class="o">.</span><span class="n">head</span><span class="p">())</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;---------------------&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Valores reconstruidos&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;---------------------&#39;</span><span class="p">)</span>
<span class="n">display</span><span class="p">(</span><span class="n">datos</span><span class="o">.</span><span class="n">head</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>------------------
Valores originales
------------------
</pre></div>
</div>
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Murder</th>
      <th>Assault</th>
      <th>UrbanPop</th>
      <th>Rape</th>
    </tr>
    <tr>
      <th>index</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Alabama</th>
      <td>13.2</td>
      <td>236.0</td>
      <td>58.0</td>
      <td>21.2</td>
    </tr>
    <tr>
      <th>Alaska</th>
      <td>10.0</td>
      <td>263.0</td>
      <td>48.0</td>
      <td>44.5</td>
    </tr>
    <tr>
      <th>Arizona</th>
      <td>8.1</td>
      <td>294.0</td>
      <td>80.0</td>
      <td>31.0</td>
    </tr>
    <tr>
      <th>Arkansas</th>
      <td>8.8</td>
      <td>190.0</td>
      <td>50.0</td>
      <td>19.5</td>
    </tr>
    <tr>
      <th>California</th>
      <td>9.0</td>
      <td>276.0</td>
      <td>91.0</td>
      <td>40.6</td>
    </tr>
  </tbody>
</table>
</div></div><div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>---------------------
Valores reconstruidos
---------------------
</pre></div>
</div>
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Murder</th>
      <th>Assault</th>
      <th>UrbanPop</th>
      <th>Rape</th>
    </tr>
    <tr>
      <th>index</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Alabama</th>
      <td>13.2</td>
      <td>236</td>
      <td>58</td>
      <td>21.2</td>
    </tr>
    <tr>
      <th>Alaska</th>
      <td>10.0</td>
      <td>263</td>
      <td>48</td>
      <td>44.5</td>
    </tr>
    <tr>
      <th>Arizona</th>
      <td>8.1</td>
      <td>294</td>
      <td>80</td>
      <td>31.0</td>
    </tr>
    <tr>
      <th>Arkansas</th>
      <td>8.8</td>
      <td>190</td>
      <td>50</td>
      <td>19.5</td>
    </tr>
    <tr>
      <th>California</th>
      <td>9.0</td>
      <td>276</td>
      <td>91</td>
      <td>40.6</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
</div>
</div>
<div class="section" id="t-distributed-stochastic-neighbor-embedding-t-sne">
<h2>t-Distributed Stochastic Neighbor Embedding (t-SNE)<a class="headerlink" href="#t-distributed-stochastic-neighbor-embedding-t-sne" title="Permalink to this headline">¬∂</a></h2>
<p>t-Distributed Stochastic Neighbor Embedding (t-SNE)  es una t√©cnica no lineal no supervisada utilizada principalmente para la exploraci√≥n de datos y la visualizaci√≥n de datos de alta dimensi√≥n.</p>
<p>En t√©rminos m√°s simples, tSNE le da una sensaci√≥n o intuici√≥n de c√≥mo se organizan los datos en un espacio de alta dimensi√≥n. Fue desarrollado por Laurens van der Maatens y Geoffrey Hinton en 2008.</p>
<div class="section" id="comparando-con-pca">
<h3>Comparando con PCA<a class="headerlink" href="#comparando-con-pca" title="Permalink to this headline">¬∂</a></h3>
<p>Si est√° familiarizado con An√°lisis de componentes principales (PCA), entonces como yo , probablemente se est√© preguntando la diferencia entre PCA y tSNE.</p>
<p>Lo primero a tener en cuenta es que PCA se desarroll√≥ en 1933, mientras que tSNE se desarroll√≥ en 2008. Mucho ha cambiado en el mundo de la ciencia de datos desde 1933, principalmente en el √°mbito del c√°lculo y el tama√±o de los datos.</p>
<p>En segundo lugar, PCA es una t√©cnica de reducci√≥n de dimensi√≥n lineal que busca maximizar la varianza y preserva las distancias pares grandes. En otras palabras, las cosas que son diferentes terminan muy separadas. Esto puede conducir a una visualizaci√≥n deficiente, especialmente cuando se trata de estructuras distribuidoras no lineales. Piense en una estructura m√∫ltiple como cualquier forma geom√©trica como: cilindro, bola, curva, etc.</p>
<p>tSNE difiere de PCA al preservar solo peque√±as distancias por pares o similitudes locales, mientras que PCA se preocupa por preservar distancias pares grandes para maximizar la varianza.</p>
<p>Laurens ilustra bastante bien el enfoque PCA y tSNE utilizando el conjunto de datos Swiss Roll en la Figura 1 [1].</p>
<p>Puede ver que debido a la no linealidad de este conjunto de datos de juguete (m√∫ltiple) y la preservaci√≥n de grandes distancias, PCA conservar√≠a incorrectamente la estructura de los datos.</p>
<img alt="../../../../_images/tsne_01.png" class="align-center" src="../../../../_images/tsne_01.png" />
<blockquote>
<div><p>Figura 1 ‚Äì Dataset de rollo suizo. Conservar la distancia peque√±a con tSNE (l√≠nea continua) frente a la maximizaci√≥n de la variaci√≥n PCA [1]</p>
</div></blockquote>
</div>
<div class="section" id="explicacion">
<h3>Explicaci√≥n<a class="headerlink" href="#explicacion" title="Permalink to this headline">¬∂</a></h3>
<p>Ahora que sabemos por qu√© podr√≠amos usar tSNE sobre PCA, analicemos c√≥mo funciona tSNE. El algoritmo tSNE calcula una medida de similitud entre pares de instancias en el espacio de alta dimensi√≥n y en el espacio de baja dimensi√≥n. Luego trata de optimizar estas dos medidas de similitud usando una funci√≥n de costo. Vamos a dividirlo en 3 pasos b√°sicos.</p>
<ol class="simple">
<li><p>Paso 1, mide similitudes entre puntos en el espacio de alta dimensi√≥n. Piense en un conjunto de puntos de datos dispersos en un espacio 2D (Figura 2).</p></li>
</ol>
<p>Para cada punto de datos (xi) centraremos una distribuci√≥n Gaussiana sobre ese punto. Luego medimos la densidad de todos los puntos (xj) bajo esa distribuci√≥n Gaussiana. Luego renormalize para todos los puntos.</p>
<p>Esto nos da un conjunto de probabilidades (Pij) para todos los puntos. Esas probabilidades son proporcionales a las similitudes.</p>
<p>Todo lo que eso significa es que si los puntos de datos x1 y x2 tienen valores iguales bajo este c√≠rculo gaussiano, entonces sus proporciones y similitudes son iguales y, por lo tanto, tienes similitudes locales en la estructura de este espacio de alta dimensi√≥n.</p>
<p>La distribuci√≥n gaussiana o el c√≠rculo se pueden manipular usando lo que se llama perplejidad, que influye en la varianza de la distribuci√≥n (tama√±o del c√≠rculo) y esencialmente en el n√∫mero de vecinos m√°s cercanos. El rango normal para la perplejidad est√° entre 5 y 50 [2].</p>
<img alt="../../../../_images/tsne_02.png" class="align-center" src="../../../../_images/tsne_02.png" />
<blockquote>
<div><p>Figura 2 ‚Äì Medici√≥n de similitudes por pares en el espacio de alta dimensi√≥n</p>
</div></blockquote>
<p>Figura 2 ‚Äì Medici√≥n de similitudes por pares en el espacio de alta dimensi√≥n
2. El paso 2 es similar al paso 1, pero en lugar de usar una distribuci√≥n gaussiana se usa una distribuci√≥n t de Student con un grado de libertad, que tambi√©n se conoce como la distribuci√≥n de Cauchy (Figura 3). Esto nos da un segundo conjunto de probabilidades (<span class="math notranslate nohighlight">\(Q_{ij}\)</span>) en el espacio de baja dimensi√≥n.</p>
<p>Como puede ver, la distribuci√≥n t de Student tiene colas m√°s pesadas que la distribuci√≥n normal. Las colas pesadas permiten un mejor modelado de distancias muy separadas.</p>
<img alt="../../../../_images/tsne_03.png" class="align-center" src="../../../../_images/tsne_03.png" />
<blockquote>
<div><p>Figura 3 ‚Äì Distribuci√≥n noraml vs t-student</p>
</div></blockquote>
<ol class="simple">
<li><p>El √∫ltimo paso es que queremos que este conjunto de probabilidades del espacio de baja dimensi√≥n (<span class="math notranslate nohighlight">\(Q_{ij}\)</span>) refleje las del espacio de alta dimensi√≥n (<span class="math notranslate nohighlight">\(P_{ij}\)</span>) de la mejor manera posible.</p></li>
</ol>
<p>Queremos que las dos estructuras de mapa sean similares. Medimos la diferencia entre las distribuciones de probabilidad de los espacios bidimensionales utilizando la divergencia de Kullback-Liebler (KL).</p>
<p>No incluir√© mucho en KL, excepto que es un enfoque asim√©trico que compara de manera eficiente los grandes valores <span class="math notranslate nohighlight">\(P_{ij}\)</span> y <span class="math notranslate nohighlight">\(Q_{ij}\)</span>. Finalmente, utilizamos el descenso de gradiente para minimizar nuestra funci√≥n de costo KL.</p>
</div>
<div class="section" id="id1">
<h3>Aplicaci√≥n<a class="headerlink" href="#id1" title="Permalink to this headline">¬∂</a></h3>
<p>Laurens van der Maaten menciona el uso de tSNE en √°reas como investigaci√≥n del clima, seguridad inform√°tica, bioinform√°tica, investigaci√≥n del c√°ncer, etc. tSNE podr√≠a usarse en datos de alta dimensi√≥n y luego el resultado de esas dimensiones se convierte en insumos para alg√∫n otro modelo de clasificaci√≥n .</p>
<p>Adem√°s, tSNE podr√≠a usarse para investigar, aprender o evaluar la segmentaci√≥n. Muchas veces seleccionamos la cantidad de segmentos antes del modelado o iteramos despu√©s de los resultados. tSNE a menudo puede mostrar una separaci√≥n clara en los datos.</p>
<p>Esto se puede usar antes de usar su modelo de segmentaci√≥n para seleccionar un n√∫mero de cl√∫ster o despu√©s para evaluar si sus segmentos realmente se mantienen. tSNE, sin embargo, no es un enfoque de agrupamiento, ya que no conserva las entradas como PCA y los valores a menudo pueden cambiar entre ejecuciones, por lo que es pura exploraci√≥n.</p>
<p>A continuaci√≥n se procede a comparar de manera  visual los algoritmos de PCA y tSNE en el conjunto de datos <code class="docutils literal notranslate"><span class="pre">Digits</span></code> .</p>
<blockquote>
<div><p><strong>Datos</strong>: El conjunto de datos contiene im√°genes de d√≠gitos escritos a mano: 10 clases donde cada clase se refiere a un d√≠gito. Los programas de preprocesamiento puestos a disposici√≥n por NIST se utilizaron para extraer mapas de bits normalizados de d√≠gitos escritos a mano de un formulario preimpreso. De un total de 43 personas, 30 contribuyeron al conjunto de entrenamiento y diferentes 13 al conjunto de prueba. Los mapas de bits de 32x32 se dividen en bloques no superpuestos de 4x4 y se cuenta el n√∫mero de p√≠xeles en cada bloque. Esto genera una matriz de entrada de 8x8 donde cada elemento es un n√∫mero entero en el rango 0‚Ä¶16. Esto reduce la dimensionalidad y da invariancia a peque√±as distorsiones.</p>
</div></blockquote>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Load Python Libraries</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">from</span> <span class="nn">time</span> <span class="kn">import</span> <span class="n">time</span>
<span class="o">%</span><span class="k">matplotlib</span> inline
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_digits</span>
<span class="kn">from</span> <span class="nn">sklearn.manifold</span> <span class="kn">import</span> <span class="n">TSNE</span>
<span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">digits</span> <span class="o">=</span> <span class="n">load_digits</span><span class="p">()</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">digits</span><span class="p">[</span><span class="s1">&#39;data&#39;</span><span class="p">])</span>
<span class="n">df</span><span class="p">[</span><span class="s1">&#39;label&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">digits</span><span class="p">[</span><span class="s1">&#39;target&#39;</span><span class="p">]</span>
<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
      <th>1</th>
      <th>2</th>
      <th>3</th>
      <th>4</th>
      <th>5</th>
      <th>6</th>
      <th>7</th>
      <th>8</th>
      <th>9</th>
      <th>10</th>
      <th>11</th>
      <th>12</th>
      <th>13</th>
      <th>14</th>
      <th>15</th>
      <th>16</th>
      <th>17</th>
      <th>18</th>
      <th>19</th>
      <th>20</th>
      <th>21</th>
      <th>22</th>
      <th>23</th>
      <th>24</th>
      <th>25</th>
      <th>26</th>
      <th>27</th>
      <th>28</th>
      <th>29</th>
      <th>30</th>
      <th>31</th>
      <th>32</th>
      <th>33</th>
      <th>34</th>
      <th>35</th>
      <th>36</th>
      <th>37</th>
      <th>38</th>
      <th>39</th>
      <th>40</th>
      <th>41</th>
      <th>42</th>
      <th>43</th>
      <th>44</th>
      <th>45</th>
      <th>46</th>
      <th>47</th>
      <th>48</th>
      <th>49</th>
      <th>50</th>
      <th>51</th>
      <th>52</th>
      <th>53</th>
      <th>54</th>
      <th>55</th>
      <th>56</th>
      <th>57</th>
      <th>58</th>
      <th>59</th>
      <th>60</th>
      <th>61</th>
      <th>62</th>
      <th>63</th>
      <th>label</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>5.0</td>
      <td>13.0</td>
      <td>9.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>13.0</td>
      <td>15.0</td>
      <td>10.0</td>
      <td>15.0</td>
      <td>5.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>3.0</td>
      <td>15.0</td>
      <td>2.0</td>
      <td>0.0</td>
      <td>11.0</td>
      <td>8.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>4.0</td>
      <td>12.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>8.0</td>
      <td>8.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>5.0</td>
      <td>8.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>9.0</td>
      <td>8.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>4.0</td>
      <td>11.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>12.0</td>
      <td>7.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>2.0</td>
      <td>14.0</td>
      <td>5.0</td>
      <td>10.0</td>
      <td>12.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>6.0</td>
      <td>13.0</td>
      <td>10.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>12.0</td>
      <td>13.0</td>
      <td>5.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>11.0</td>
      <td>16.0</td>
      <td>9.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>3.0</td>
      <td>15.0</td>
      <td>16.0</td>
      <td>6.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>7.0</td>
      <td>15.0</td>
      <td>16.0</td>
      <td>16.0</td>
      <td>2.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>16.0</td>
      <td>16.0</td>
      <td>3.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>16.0</td>
      <td>16.0</td>
      <td>6.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>16.0</td>
      <td>16.0</td>
      <td>6.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>11.0</td>
      <td>16.0</td>
      <td>10.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>4.0</td>
      <td>15.0</td>
      <td>12.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>3.0</td>
      <td>16.0</td>
      <td>15.0</td>
      <td>14.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>8.0</td>
      <td>13.0</td>
      <td>8.0</td>
      <td>16.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>6.0</td>
      <td>15.0</td>
      <td>11.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>8.0</td>
      <td>13.0</td>
      <td>15.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>9.0</td>
      <td>16.0</td>
      <td>16.0</td>
      <td>5.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>3.0</td>
      <td>13.0</td>
      <td>16.0</td>
      <td>16.0</td>
      <td>11.0</td>
      <td>5.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>3.0</td>
      <td>11.0</td>
      <td>16.0</td>
      <td>9.0</td>
      <td>0.0</td>
      <td>2</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>7.0</td>
      <td>15.0</td>
      <td>13.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>8.0</td>
      <td>13.0</td>
      <td>6.0</td>
      <td>15.0</td>
      <td>4.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>2.0</td>
      <td>1.0</td>
      <td>13.0</td>
      <td>13.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>2.0</td>
      <td>15.0</td>
      <td>11.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>12.0</td>
      <td>12.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>10.0</td>
      <td>8.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>8.0</td>
      <td>4.0</td>
      <td>5.0</td>
      <td>14.0</td>
      <td>9.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>7.0</td>
      <td>13.0</td>
      <td>13.0</td>
      <td>9.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>11.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>7.0</td>
      <td>8.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>13.0</td>
      <td>6.0</td>
      <td>2.0</td>
      <td>2.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>7.0</td>
      <td>15.0</td>
      <td>0.0</td>
      <td>9.0</td>
      <td>8.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>5.0</td>
      <td>16.0</td>
      <td>10.0</td>
      <td>0.0</td>
      <td>16.0</td>
      <td>6.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>4.0</td>
      <td>15.0</td>
      <td>16.0</td>
      <td>13.0</td>
      <td>16.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>3.0</td>
      <td>15.0</td>
      <td>10.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>2.0</td>
      <td>16.0</td>
      <td>4.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>4</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># PCA</span>
<span class="n">scaler</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="s1">&#39;label&#39;</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;label&#39;</span><span class="p">]</span>
    
<span class="n">embedding</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">X_transform</span> <span class="o">=</span> <span class="n">embedding</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    
<span class="n">df_pca</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">X_transform</span><span class="p">,</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Score1&#39;</span><span class="p">,</span><span class="s1">&#39;Score2&#39;</span><span class="p">])</span>
<span class="n">df_pca</span><span class="p">[</span><span class="s1">&#39;label&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">y</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Plot Digits PCA</span>


<span class="c1"># Set style of scatterplot</span>
<span class="n">sns</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="s2">&quot;notebook&quot;</span><span class="p">,</span> <span class="n">font_scale</span><span class="o">=</span><span class="mf">1.1</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">set_style</span><span class="p">(</span><span class="s2">&quot;ticks&quot;</span><span class="p">)</span>

<span class="c1"># Create scatterplot of dataframe</span>
<span class="n">sns</span><span class="o">.</span><span class="n">lmplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s1">&#39;Score1&#39;</span><span class="p">,</span>
           <span class="n">y</span><span class="o">=</span><span class="s1">&#39;Score2&#39;</span><span class="p">,</span>
           <span class="n">data</span><span class="o">=</span><span class="n">df_pca</span><span class="p">,</span>
           <span class="n">fit_reg</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
           <span class="n">legend</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
           <span class="n">height</span><span class="o">=</span><span class="mi">9</span><span class="p">,</span>
           <span class="n">hue</span><span class="o">=</span><span class="s1">&#39;label&#39;</span><span class="p">,</span>
           <span class="n">scatter_kws</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;s&quot;</span><span class="p">:</span><span class="mi">200</span><span class="p">,</span> <span class="s2">&quot;alpha&quot;</span><span class="p">:</span><span class="mf">0.3</span><span class="p">})</span>

<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;PCA Results: Digits&#39;</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">set_fontsize</span><span class="p">(</span><span class="s1">&#39;14&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Prin Comp 1&#39;</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">set_fontsize</span><span class="p">(</span><span class="s1">&#39;10&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Prin Comp 2&#39;</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">set_fontsize</span><span class="p">(</span><span class="s1">&#39;10&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../../_images/03_reduccion_dimensionalidad_43_0.png" src="../../../../_images/03_reduccion_dimensionalidad_43_0.png" />
</div>
</div>
<p>Al graficar las dos componentes principales del m√©todo PCA, se observa que no existe una clara distinci√≥n entre la distintas clases (solo se ve un gran c√∫mulo de puntos mezclados).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># tsne</span>
<span class="n">scaler</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="s1">&#39;label&#39;</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;label&#39;</span><span class="p">]</span>
    
<span class="n">embedding</span> <span class="o">=</span> <span class="n">TSNE</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">X_transform</span> <span class="o">=</span> <span class="n">embedding</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    
<span class="n">df_tsne</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">X_transform</span><span class="p">,</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;_DIM_1_&#39;</span><span class="p">,</span><span class="s1">&#39;_DIM_2_&#39;</span><span class="p">])</span>
<span class="n">df_tsne</span><span class="p">[</span><span class="s1">&#39;label&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">y</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Plot Digits t-SNE</span>
<span class="n">sns</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="s2">&quot;notebook&quot;</span><span class="p">,</span> <span class="n">font_scale</span><span class="o">=</span><span class="mf">1.1</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">set_style</span><span class="p">(</span><span class="s2">&quot;ticks&quot;</span><span class="p">)</span>

<span class="n">sns</span><span class="o">.</span><span class="n">lmplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s1">&#39;_DIM_1_&#39;</span><span class="p">,</span>
           <span class="n">y</span><span class="o">=</span><span class="s1">&#39;_DIM_2_&#39;</span><span class="p">,</span>
           <span class="n">data</span><span class="o">=</span><span class="n">df_tsne</span><span class="p">,</span>
           <span class="n">fit_reg</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
           <span class="n">legend</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
           <span class="n">height</span><span class="o">=</span><span class="mi">9</span><span class="p">,</span>
           <span class="n">hue</span><span class="o">=</span><span class="s1">&#39;label&#39;</span><span class="p">,</span>
           <span class="n">scatter_kws</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;s&quot;</span><span class="p">:</span><span class="mi">200</span><span class="p">,</span> <span class="s2">&quot;alpha&quot;</span><span class="p">:</span><span class="mf">0.3</span><span class="p">})</span>

<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;t-SNE Results: Digits&#39;</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">set_fontsize</span><span class="p">(</span><span class="s1">&#39;14&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Dimension 1&#39;</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">set_fontsize</span><span class="p">(</span><span class="s1">&#39;10&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Dimension 2&#39;</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="s1">&#39;bold&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">set_fontsize</span><span class="p">(</span><span class="s1">&#39;10&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../../../_images/03_reduccion_dimensionalidad_46_0.png" src="../../../../_images/03_reduccion_dimensionalidad_46_0.png" />
</div>
</div>
<p>Para el caso del m√©todo TSNE, se observa una diferenciaci√≥n entre los grupos de estudios (aspecto que fue muy distinto al momento de analizar el m√©todo del PCA).</p>
<blockquote>
<div><p><strong>Observaci√≥n</strong>: Si bien se muestra donde el m√©todo TSNE logra ser superior en aspecto de reducci√≥n de dimensionalidad que el m√©todo PCA, no significa que para distintos experimientos se tengan los mismo resultados.</p>
</div></blockquote>
</div>
</div>
<div class="section" id="otros-metodos-de-reduccion-de-dimensionalidad">
<h2>Otros m√©todos de reducci√≥n de dimensionalidad<a class="headerlink" href="#otros-metodos-de-reduccion-de-dimensionalidad" title="Permalink to this headline">¬∂</a></h2>
<p>Existen otro m√©todos de reducci√≥n de dimencionalidad, a continuaci√≥n se deja una referencia con la descripci√≥n de cada uno de estos algoritmos.</p>
<ul class="simple">
<li><p><a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html">Descomposici√≥n del valor singular </a></p></li>
<li><p><a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.NMF.html">Non-Negative Matrix Factorization </a></p></li>
<li><p><a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.manifold.Isomap.html">Isomap Embedding </a></p></li>
<li><p><a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.manifold.LocallyLinearEmbedding.html">Locally Linear Embedding </a></p></li>
<li><p><a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.manifold.MDS.html">Multidimensional Scaling </a></p></li>
<li><p><a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.manifold.SpectralEmbedding.html">Spectral Embedding </a></p></li>
</ul>
<div class="section" id="id2">
<h3>Aplicaci√≥n<a class="headerlink" href="#id2" title="Permalink to this headline">¬∂</a></h3>
<p>En este ejemplo se quiere aprovechar las bondades de aplicar la reducci√≥n de dimensionalidad para ocupar un modelo de clasificaci√≥n (en este caso, el modelo de regresi√≥n log√≠stica). Para ello se ocupar√° el conjunto de datos <code class="docutils literal notranslate"><span class="pre">meatspec.csv</span></code></p>
<blockquote>
<div><p><strong>Datos</strong>: El departamento de calidad de una empresa de alimentaci√≥n se encarga de medir el contenido en grasa de la carne que comercializa. Este estudio se realiza mediante t√©cnicas de anal√≠tica qu√≠mica, un proceso relativamente costoso en tiempo y recursos. Una alternativa que permitir√≠a reducir costes y optimizar tiempo es emplear un espectrofot√≥metro (instrumento capaz de detectar la absorbancia que tiene un material a diferentes tipos de luz en funci√≥n de sus caracter√≠sticas) e inferir el contenido en grasa a partir de sus medidas.</p>
</div></blockquote>
<blockquote>
<div><p>Antes de dar por v√°lida esta nueva t√©cnica, la empresa necesita comprobar qu√© margen de error tiene respecto al an√°lisis qu√≠mico. Para ello, se mide el espectro de absorbancia a 100 longitudes de onda en 215 muestras de carne, cuyo contenido en grasa se obtiene tambi√©n por an√°lisis qu√≠mico, y se entrena un modelo con el objetivo de predecir el contenido en grasa a partir de los valores dados por el espectrofot√≥metro.</p>
</div></blockquote>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Librerias</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">PCA</span><span class="p">,</span><span class="n">TruncatedSVD</span><span class="p">,</span><span class="n">NMF</span>
<span class="kn">from</span> <span class="nn">sklearn.manifold</span> <span class="kn">import</span> <span class="n">Isomap</span><span class="p">,</span><span class="n">LocallyLinearEmbedding</span><span class="p">,</span><span class="n">MDS</span><span class="p">,</span><span class="n">SpectralEmbedding</span><span class="p">,</span><span class="n">TSNE</span>

<span class="kn">from</span> <span class="nn">metrics_regression</span> <span class="kn">import</span> <span class="n">summary_metrics</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Datos</span>
<span class="n">datos</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;data/meatspec.csv&#39;</span><span class="p">)</span>
<span class="n">datos</span> <span class="o">=</span> <span class="n">datos</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span> <span class="o">=</span> <span class="n">datos</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">datos</span><span class="p">[</span><span class="s1">&#39;fat&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">datos</span><span class="p">[</span><span class="s1">&#39;fat&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">float</span><span class="p">)</span>
<span class="n">datos</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>V1</th>
      <th>V2</th>
      <th>V3</th>
      <th>V4</th>
      <th>V5</th>
      <th>V6</th>
      <th>V7</th>
      <th>V8</th>
      <th>V9</th>
      <th>V10</th>
      <th>V11</th>
      <th>V12</th>
      <th>V13</th>
      <th>V14</th>
      <th>V15</th>
      <th>V16</th>
      <th>V17</th>
      <th>V18</th>
      <th>V19</th>
      <th>V20</th>
      <th>V21</th>
      <th>V22</th>
      <th>V23</th>
      <th>V24</th>
      <th>V25</th>
      <th>V26</th>
      <th>V27</th>
      <th>V28</th>
      <th>V29</th>
      <th>V30</th>
      <th>V31</th>
      <th>V32</th>
      <th>V33</th>
      <th>V34</th>
      <th>V35</th>
      <th>V36</th>
      <th>V37</th>
      <th>V38</th>
      <th>V39</th>
      <th>V40</th>
      <th>V41</th>
      <th>V42</th>
      <th>V43</th>
      <th>V44</th>
      <th>V45</th>
      <th>V46</th>
      <th>V47</th>
      <th>V48</th>
      <th>V49</th>
      <th>V50</th>
      <th>V51</th>
      <th>V52</th>
      <th>V53</th>
      <th>V54</th>
      <th>V55</th>
      <th>V56</th>
      <th>V57</th>
      <th>V58</th>
      <th>V59</th>
      <th>V60</th>
      <th>V61</th>
      <th>V62</th>
      <th>V63</th>
      <th>V64</th>
      <th>V65</th>
      <th>V66</th>
      <th>V67</th>
      <th>V68</th>
      <th>V69</th>
      <th>V70</th>
      <th>V71</th>
      <th>V72</th>
      <th>V73</th>
      <th>V74</th>
      <th>V75</th>
      <th>V76</th>
      <th>V77</th>
      <th>V78</th>
      <th>V79</th>
      <th>V80</th>
      <th>V81</th>
      <th>V82</th>
      <th>V83</th>
      <th>V84</th>
      <th>V85</th>
      <th>V86</th>
      <th>V87</th>
      <th>V88</th>
      <th>V89</th>
      <th>V90</th>
      <th>V91</th>
      <th>V92</th>
      <th>V93</th>
      <th>V94</th>
      <th>V95</th>
      <th>V96</th>
      <th>V97</th>
      <th>V98</th>
      <th>V99</th>
      <th>V100</th>
      <th>fat</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>2.61776</td>
      <td>2.61814</td>
      <td>2.61859</td>
      <td>2.61912</td>
      <td>2.61981</td>
      <td>2.62071</td>
      <td>2.62186</td>
      <td>2.62334</td>
      <td>2.62511</td>
      <td>2.62722</td>
      <td>2.62964</td>
      <td>2.63245</td>
      <td>2.63565</td>
      <td>2.63933</td>
      <td>2.64353</td>
      <td>2.64825</td>
      <td>2.65350</td>
      <td>2.65937</td>
      <td>2.66585</td>
      <td>2.67281</td>
      <td>2.68008</td>
      <td>2.68733</td>
      <td>2.69427</td>
      <td>2.70073</td>
      <td>2.70684</td>
      <td>2.71281</td>
      <td>2.71914</td>
      <td>2.72628</td>
      <td>2.73462</td>
      <td>2.74416</td>
      <td>2.75466</td>
      <td>2.76568</td>
      <td>2.77679</td>
      <td>2.78790</td>
      <td>2.79949</td>
      <td>2.81225</td>
      <td>2.82706</td>
      <td>2.84356</td>
      <td>2.86106</td>
      <td>2.87857</td>
      <td>2.89497</td>
      <td>2.90924</td>
      <td>2.92085</td>
      <td>2.93015</td>
      <td>2.93846</td>
      <td>2.94771</td>
      <td>2.96019</td>
      <td>2.97831</td>
      <td>3.00306</td>
      <td>3.03506</td>
      <td>3.07428</td>
      <td>3.11963</td>
      <td>3.16868</td>
      <td>3.21771</td>
      <td>3.26254</td>
      <td>3.29988</td>
      <td>3.32847</td>
      <td>3.34899</td>
      <td>3.36342</td>
      <td>3.37379</td>
      <td>3.38152</td>
      <td>3.38741</td>
      <td>3.39164</td>
      <td>3.39418</td>
      <td>3.39490</td>
      <td>3.39366</td>
      <td>3.39045</td>
      <td>3.38541</td>
      <td>3.37869</td>
      <td>3.37041</td>
      <td>3.36073</td>
      <td>3.34979</td>
      <td>3.33769</td>
      <td>3.32443</td>
      <td>3.31013</td>
      <td>3.29487</td>
      <td>3.27891</td>
      <td>3.26232</td>
      <td>3.24542</td>
      <td>3.22828</td>
      <td>3.21080</td>
      <td>3.19287</td>
      <td>3.17433</td>
      <td>3.15503</td>
      <td>3.13475</td>
      <td>3.11339</td>
      <td>3.09116</td>
      <td>3.06850</td>
      <td>3.04596</td>
      <td>3.02393</td>
      <td>3.00247</td>
      <td>2.98145</td>
      <td>2.96072</td>
      <td>2.94013</td>
      <td>2.91978</td>
      <td>2.89966</td>
      <td>2.87964</td>
      <td>2.85960</td>
      <td>2.83940</td>
      <td>2.81920</td>
      <td>22.5</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2.83454</td>
      <td>2.83871</td>
      <td>2.84283</td>
      <td>2.84705</td>
      <td>2.85138</td>
      <td>2.85587</td>
      <td>2.86060</td>
      <td>2.86566</td>
      <td>2.87093</td>
      <td>2.87661</td>
      <td>2.88264</td>
      <td>2.88898</td>
      <td>2.89577</td>
      <td>2.90308</td>
      <td>2.91097</td>
      <td>2.91953</td>
      <td>2.92873</td>
      <td>2.93863</td>
      <td>2.94929</td>
      <td>2.96072</td>
      <td>2.97272</td>
      <td>2.98493</td>
      <td>2.99690</td>
      <td>3.00833</td>
      <td>3.01920</td>
      <td>3.02990</td>
      <td>3.04101</td>
      <td>3.05345</td>
      <td>3.06777</td>
      <td>3.08416</td>
      <td>3.10221</td>
      <td>3.12106</td>
      <td>3.13983</td>
      <td>3.15810</td>
      <td>3.17623</td>
      <td>3.19519</td>
      <td>3.21584</td>
      <td>3.23747</td>
      <td>3.25889</td>
      <td>3.27835</td>
      <td>3.29384</td>
      <td>3.30362</td>
      <td>3.30681</td>
      <td>3.30393</td>
      <td>3.29700</td>
      <td>3.28925</td>
      <td>3.28409</td>
      <td>3.28505</td>
      <td>3.29326</td>
      <td>3.30923</td>
      <td>3.33267</td>
      <td>3.36251</td>
      <td>3.39661</td>
      <td>3.43188</td>
      <td>3.46492</td>
      <td>3.49295</td>
      <td>3.51458</td>
      <td>3.53004</td>
      <td>3.54067</td>
      <td>3.54797</td>
      <td>3.55306</td>
      <td>3.55675</td>
      <td>3.55921</td>
      <td>3.56045</td>
      <td>3.56034</td>
      <td>3.55876</td>
      <td>3.55571</td>
      <td>3.55132</td>
      <td>3.54585</td>
      <td>3.53950</td>
      <td>3.53235</td>
      <td>3.52442</td>
      <td>3.51583</td>
      <td>3.50668</td>
      <td>3.49700</td>
      <td>3.48683</td>
      <td>3.47626</td>
      <td>3.46552</td>
      <td>3.45501</td>
      <td>3.44481</td>
      <td>3.43477</td>
      <td>3.42465</td>
      <td>3.41419</td>
      <td>3.40303</td>
      <td>3.39082</td>
      <td>3.37731</td>
      <td>3.36265</td>
      <td>3.34745</td>
      <td>3.33245</td>
      <td>3.31818</td>
      <td>3.30473</td>
      <td>3.29186</td>
      <td>3.27921</td>
      <td>3.26655</td>
      <td>3.25369</td>
      <td>3.24045</td>
      <td>3.22659</td>
      <td>3.21181</td>
      <td>3.19600</td>
      <td>3.17942</td>
      <td>40.1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2.58284</td>
      <td>2.58458</td>
      <td>2.58629</td>
      <td>2.58808</td>
      <td>2.58996</td>
      <td>2.59192</td>
      <td>2.59401</td>
      <td>2.59627</td>
      <td>2.59873</td>
      <td>2.60131</td>
      <td>2.60414</td>
      <td>2.60714</td>
      <td>2.61029</td>
      <td>2.61361</td>
      <td>2.61714</td>
      <td>2.62089</td>
      <td>2.62486</td>
      <td>2.62909</td>
      <td>2.63361</td>
      <td>2.63835</td>
      <td>2.64330</td>
      <td>2.64838</td>
      <td>2.65354</td>
      <td>2.65870</td>
      <td>2.66375</td>
      <td>2.66880</td>
      <td>2.67383</td>
      <td>2.67892</td>
      <td>2.68411</td>
      <td>2.68937</td>
      <td>2.69470</td>
      <td>2.70012</td>
      <td>2.70563</td>
      <td>2.71141</td>
      <td>2.71775</td>
      <td>2.72490</td>
      <td>2.73344</td>
      <td>2.74327</td>
      <td>2.75433</td>
      <td>2.76642</td>
      <td>2.77931</td>
      <td>2.79272</td>
      <td>2.80649</td>
      <td>2.82064</td>
      <td>2.83541</td>
      <td>2.85121</td>
      <td>2.86872</td>
      <td>2.88905</td>
      <td>2.91289</td>
      <td>2.94088</td>
      <td>2.97325</td>
      <td>3.00946</td>
      <td>3.04780</td>
      <td>3.08554</td>
      <td>3.11947</td>
      <td>3.14696</td>
      <td>3.16677</td>
      <td>3.17938</td>
      <td>3.18631</td>
      <td>3.18924</td>
      <td>3.18950</td>
      <td>3.18801</td>
      <td>3.18498</td>
      <td>3.18039</td>
      <td>3.17411</td>
      <td>3.16611</td>
      <td>3.15641</td>
      <td>3.14512</td>
      <td>3.13241</td>
      <td>3.11843</td>
      <td>3.10329</td>
      <td>3.08714</td>
      <td>3.07014</td>
      <td>3.05237</td>
      <td>3.03393</td>
      <td>3.01504</td>
      <td>2.99569</td>
      <td>2.97612</td>
      <td>2.95642</td>
      <td>2.93660</td>
      <td>2.91667</td>
      <td>2.89655</td>
      <td>2.87622</td>
      <td>2.85563</td>
      <td>2.83474</td>
      <td>2.81361</td>
      <td>2.79235</td>
      <td>2.77113</td>
      <td>2.75015</td>
      <td>2.72956</td>
      <td>2.70934</td>
      <td>2.68951</td>
      <td>2.67009</td>
      <td>2.65112</td>
      <td>2.63262</td>
      <td>2.61461</td>
      <td>2.59718</td>
      <td>2.58034</td>
      <td>2.56404</td>
      <td>2.54816</td>
      <td>8.4</td>
    </tr>
    <tr>
      <th>3</th>
      <td>2.82286</td>
      <td>2.82460</td>
      <td>2.82630</td>
      <td>2.82814</td>
      <td>2.83001</td>
      <td>2.83192</td>
      <td>2.83392</td>
      <td>2.83606</td>
      <td>2.83842</td>
      <td>2.84097</td>
      <td>2.84374</td>
      <td>2.84664</td>
      <td>2.84975</td>
      <td>2.85307</td>
      <td>2.85661</td>
      <td>2.86038</td>
      <td>2.86437</td>
      <td>2.86860</td>
      <td>2.87308</td>
      <td>2.87789</td>
      <td>2.88301</td>
      <td>2.88832</td>
      <td>2.89374</td>
      <td>2.89917</td>
      <td>2.90457</td>
      <td>2.90991</td>
      <td>2.91521</td>
      <td>2.92043</td>
      <td>2.92565</td>
      <td>2.93082</td>
      <td>2.93604</td>
      <td>2.94128</td>
      <td>2.94658</td>
      <td>2.95202</td>
      <td>2.95777</td>
      <td>2.96419</td>
      <td>2.97159</td>
      <td>2.98045</td>
      <td>2.99090</td>
      <td>3.00284</td>
      <td>3.01611</td>
      <td>3.03048</td>
      <td>3.04579</td>
      <td>3.06194</td>
      <td>3.07889</td>
      <td>3.09686</td>
      <td>3.11629</td>
      <td>3.13775</td>
      <td>3.16217</td>
      <td>3.19068</td>
      <td>3.22376</td>
      <td>3.26172</td>
      <td>3.30379</td>
      <td>3.34793</td>
      <td>3.39093</td>
      <td>3.42920</td>
      <td>3.45998</td>
      <td>3.48227</td>
      <td>3.49687</td>
      <td>3.50558</td>
      <td>3.51026</td>
      <td>3.51221</td>
      <td>3.51215</td>
      <td>3.51036</td>
      <td>3.50682</td>
      <td>3.50140</td>
      <td>3.49398</td>
      <td>3.48457</td>
      <td>3.47333</td>
      <td>3.46041</td>
      <td>3.44595</td>
      <td>3.43005</td>
      <td>3.41285</td>
      <td>3.39450</td>
      <td>3.37511</td>
      <td>3.35482</td>
      <td>3.33376</td>
      <td>3.31204</td>
      <td>3.28986</td>
      <td>3.26730</td>
      <td>3.24442</td>
      <td>3.22117</td>
      <td>3.19757</td>
      <td>3.17357</td>
      <td>3.14915</td>
      <td>3.12429</td>
      <td>3.09908</td>
      <td>3.07366</td>
      <td>3.04825</td>
      <td>3.02308</td>
      <td>2.99820</td>
      <td>2.97367</td>
      <td>2.94951</td>
      <td>2.92576</td>
      <td>2.90251</td>
      <td>2.87988</td>
      <td>2.85794</td>
      <td>2.83672</td>
      <td>2.81617</td>
      <td>2.79622</td>
      <td>5.9</td>
    </tr>
    <tr>
      <th>4</th>
      <td>2.78813</td>
      <td>2.78989</td>
      <td>2.79167</td>
      <td>2.79350</td>
      <td>2.79538</td>
      <td>2.79746</td>
      <td>2.79984</td>
      <td>2.80254</td>
      <td>2.80553</td>
      <td>2.80890</td>
      <td>2.81272</td>
      <td>2.81704</td>
      <td>2.82184</td>
      <td>2.82710</td>
      <td>2.83294</td>
      <td>2.83945</td>
      <td>2.84664</td>
      <td>2.85458</td>
      <td>2.86331</td>
      <td>2.87280</td>
      <td>2.88291</td>
      <td>2.89335</td>
      <td>2.90374</td>
      <td>2.91371</td>
      <td>2.92305</td>
      <td>2.93187</td>
      <td>2.94060</td>
      <td>2.94986</td>
      <td>2.96035</td>
      <td>2.97241</td>
      <td>2.98606</td>
      <td>3.00097</td>
      <td>3.01652</td>
      <td>3.03220</td>
      <td>3.04793</td>
      <td>3.06413</td>
      <td>3.08153</td>
      <td>3.10078</td>
      <td>3.12185</td>
      <td>3.14371</td>
      <td>3.16510</td>
      <td>3.18470</td>
      <td>3.20140</td>
      <td>3.21477</td>
      <td>3.22544</td>
      <td>3.23505</td>
      <td>3.24586</td>
      <td>3.26027</td>
      <td>3.28063</td>
      <td>3.30889</td>
      <td>3.34543</td>
      <td>3.39019</td>
      <td>3.44198</td>
      <td>3.49800</td>
      <td>3.55407</td>
      <td>3.60534</td>
      <td>3.64789</td>
      <td>3.68011</td>
      <td>3.70272</td>
      <td>3.71815</td>
      <td>3.72863</td>
      <td>3.73574</td>
      <td>3.74059</td>
      <td>3.74357</td>
      <td>3.74453</td>
      <td>3.74336</td>
      <td>3.73991</td>
      <td>3.73418</td>
      <td>3.72638</td>
      <td>3.71676</td>
      <td>3.70553</td>
      <td>3.69289</td>
      <td>3.67900</td>
      <td>3.66396</td>
      <td>3.64785</td>
      <td>3.63085</td>
      <td>3.61305</td>
      <td>3.59463</td>
      <td>3.57582</td>
      <td>3.55695</td>
      <td>3.53796</td>
      <td>3.51880</td>
      <td>3.49936</td>
      <td>3.47938</td>
      <td>3.45869</td>
      <td>3.43711</td>
      <td>3.41458</td>
      <td>3.39129</td>
      <td>3.36772</td>
      <td>3.34450</td>
      <td>3.32201</td>
      <td>3.30025</td>
      <td>3.27907</td>
      <td>3.25831</td>
      <td>3.23784</td>
      <td>3.21765</td>
      <td>3.19766</td>
      <td>3.17770</td>
      <td>3.15770</td>
      <td>3.13753</td>
      <td>25.5</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>El set de datos contiene 101 columnas. Las 100 primeras, nombradas como  <span class="math notranslate nohighlight">\(V_1,...,V_{100}\)</span>  recogen el valor de absorbancia para cada una de las 100 longitudes de onda analizadas (predictores), y la columna fat el contenido en grasa medido por t√©cnicas qu√≠micas (variable respuesta).</p>
<p>Muchas de las variables est√°n altamente correlacionadas (correlaci√≥n absoluta &gt; 0.8), lo que supone un problema a la hora de emplear modelos de regresi√≥n lineal.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Correlaci√≥n entre columnas num√©ricas</span>
<span class="k">def</span> <span class="nf">tidy_corr_matrix</span><span class="p">(</span><span class="n">corr_mat</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Funci√≥n para convertir una matriz de correlaci√≥n de pandas en formato tidy</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="n">corr_mat</span> <span class="o">=</span> <span class="n">corr_mat</span><span class="o">.</span><span class="n">stack</span><span class="p">()</span><span class="o">.</span><span class="n">reset_index</span><span class="p">()</span>
    <span class="n">corr_mat</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;variable_1&#39;</span><span class="p">,</span><span class="s1">&#39;variable_2&#39;</span><span class="p">,</span><span class="s1">&#39;r&#39;</span><span class="p">]</span>
    <span class="n">corr_mat</span> <span class="o">=</span> <span class="n">corr_mat</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">corr_mat</span><span class="p">[</span><span class="s1">&#39;variable_1&#39;</span><span class="p">]</span> <span class="o">!=</span> <span class="n">corr_mat</span><span class="p">[</span><span class="s1">&#39;variable_2&#39;</span><span class="p">],</span> <span class="p">:]</span>
    <span class="n">corr_mat</span><span class="p">[</span><span class="s1">&#39;abs_r&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">corr_mat</span><span class="p">[</span><span class="s1">&#39;r&#39;</span><span class="p">])</span>
    <span class="n">corr_mat</span> <span class="o">=</span> <span class="n">corr_mat</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="s1">&#39;abs_r&#39;</span><span class="p">,</span> <span class="n">ascending</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    
    <span class="k">return</span><span class="p">(</span><span class="n">corr_mat</span><span class="p">)</span>

<span class="n">corr_matrix</span> <span class="o">=</span> <span class="n">datos</span><span class="o">.</span><span class="n">select_dtypes</span><span class="p">(</span><span class="n">include</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;float64&#39;</span><span class="p">,</span> <span class="s1">&#39;int&#39;</span><span class="p">])</span> \
              <span class="o">.</span><span class="n">corr</span><span class="p">(</span><span class="n">method</span><span class="o">=</span><span class="s1">&#39;pearson&#39;</span><span class="p">)</span>
<span class="n">display</span><span class="p">(</span><span class="n">tidy_corr_matrix</span><span class="p">(</span><span class="n">corr_matrix</span><span class="p">)</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">5</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>variable_1</th>
      <th>variable_2</th>
      <th>r</th>
      <th>abs_r</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1019</th>
      <td>V11</td>
      <td>V10</td>
      <td>0.999996</td>
      <td>0.999996</td>
    </tr>
    <tr>
      <th>919</th>
      <td>V10</td>
      <td>V11</td>
      <td>0.999996</td>
      <td>0.999996</td>
    </tr>
    <tr>
      <th>1021</th>
      <td>V11</td>
      <td>V12</td>
      <td>0.999996</td>
      <td>0.999996</td>
    </tr>
    <tr>
      <th>1121</th>
      <td>V12</td>
      <td>V11</td>
      <td>0.999996</td>
      <td>0.999996</td>
    </tr>
    <tr>
      <th>917</th>
      <td>V10</td>
      <td>V9</td>
      <td>0.999996</td>
      <td>0.999996</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Se procede aplicar el modelo de regresi√≥n lineal .</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Divisi√≥n de los datos en train y test</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">datos</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="s1">&#39;fat&#39;</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">datos</span><span class="p">[</span><span class="s1">&#39;fat&#39;</span><span class="p">]</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
                                        <span class="n">X</span><span class="p">,</span>
                                        <span class="n">y</span><span class="o">.</span><span class="n">values</span><span class="p">,</span>
                                        <span class="n">train_size</span>   <span class="o">=</span> <span class="mf">0.7</span><span class="p">,</span>
                                        <span class="n">random_state</span> <span class="o">=</span> <span class="mi">1234</span><span class="p">,</span>
                                        <span class="n">shuffle</span>      <span class="o">=</span> <span class="kc">True</span>
                                    <span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Creaci√≥n y entrenamiento del modelo</span>
<span class="n">modelo</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">(</span><span class="n">normalize</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">modelo</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span> <span class="o">=</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">y_train</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>LinearRegression(normalize=True)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Predicciones test</span>
<span class="n">predicciones</span> <span class="o">=</span> <span class="n">modelo</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="o">=</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">predicciones</span> <span class="o">=</span> <span class="n">predicciones</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>

<span class="c1"># Error de test del modelo </span>
<span class="n">df_pred</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span>
    <span class="s1">&#39;y&#39;</span><span class="p">:</span><span class="n">y_test</span><span class="p">,</span>
    <span class="s1">&#39;yhat&#39;</span><span class="p">:</span><span class="n">predicciones</span>
<span class="p">})</span>

<span class="n">df_summary</span> <span class="o">=</span> <span class="n">summary_metrics</span><span class="p">(</span><span class="n">df_pred</span><span class="p">)</span>
<span class="n">df_summary</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>mae</th>
      <th>mse</th>
      <th>rmse</th>
      <th>mape</th>
      <th>maape</th>
      <th>wmape</th>
      <th>mmape</th>
      <th>smape</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>2.0904</td>
      <td>14.743</td>
      <td>3.8397</td>
      <td>0.1616</td>
      <td>0.1484</td>
      <td>0.1088</td>
      <td>0.1362</td>
      <td>0.1665</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Ahora se ocuparan los modelos de reducci√≥n de dimensionalidad para entrenar el modelo de regresi√≥n lineal. Para ello se ocuparan los distintos algoritmos mencionados. Lo primero es crear una funci√≥n que pueda realizar esta tarea de manera autom√°tica.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># funcion de reduccion de dimensionalidad y aplicacion de la regresion lineal</span>

<span class="k">def</span> <span class="nf">dr_pipeline</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">model_dr</span><span class="p">):</span>
    
    <span class="c1"># datos</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="s1">&#39;fat&#39;</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;fat&#39;</span><span class="p">]</span>
    
    <span class="c1"># reduccion de la dimensionalidad</span>
    <span class="n">embedding</span> <span class="o">=</span> <span class="n">model_dr</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">embedding</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    

    <span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
                                        <span class="n">X</span><span class="p">,</span>
                                        <span class="n">y</span><span class="p">,</span>
                                        <span class="n">train_size</span>   <span class="o">=</span> <span class="mf">0.7</span><span class="p">,</span>
                                        <span class="n">random_state</span> <span class="o">=</span> <span class="mi">1234</span><span class="p">,</span>
                                        <span class="n">shuffle</span>      <span class="o">=</span> <span class="kc">True</span>
                                    <span class="p">)</span>
    
    <span class="c1"># Creaci√≥n y entrenamiento del modelo</span>
    <span class="n">modelo</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">(</span><span class="n">normalize</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">modelo</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span> <span class="o">=</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">y_train</span><span class="p">)</span>

    
    <span class="c1"># Predicciones test</span>
    <span class="n">predicciones</span> <span class="o">=</span> <span class="n">modelo</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="o">=</span><span class="n">X_test</span><span class="p">)</span>
    <span class="n">predicciones</span> <span class="o">=</span> <span class="n">predicciones</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>

    <span class="c1"># Error de test del modelo </span>
    <span class="n">df_pred</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span>
        <span class="s1">&#39;y&#39;</span><span class="p">:</span><span class="n">y_test</span><span class="p">,</span>
        <span class="s1">&#39;yhat&#39;</span><span class="p">:</span><span class="n">predicciones</span>
    <span class="p">})</span>

    <span class="n">df_summary</span> <span class="o">=</span> <span class="n">summary_metrics</span><span class="p">(</span><span class="n">df_pred</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">df_summary</span>
</pre></div>
</div>
</div>
</div>
<p><strong>Enfoque: Algebra lineal</strong></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">modelos_algebra_lineal</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">(</span><span class="s1">&#39;PCA&#39;</span><span class="p">,</span><span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">5</span><span class="p">)),</span>
    <span class="p">(</span><span class="s1">&#39;SVD&#39;</span><span class="p">,</span><span class="n">TruncatedSVD</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">5</span><span class="p">)),</span>
    <span class="p">(</span><span class="s1">&#39;NMF&#39;</span><span class="p">,</span><span class="n">NMF</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">5</span><span class="p">))</span>
<span class="p">]</span>

<span class="n">names</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">modelos_algebra_lineal</span><span class="p">]</span>
<span class="n">results</span> <span class="o">=</span> <span class="p">[</span><span class="n">dr_pipeline</span><span class="p">(</span><span class="n">datos</span><span class="p">,</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">modelos_algebra_lineal</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/home/runner/.cache/pypoetry/virtualenvs/mat281-2021-V7B8LTfe-py3.8/lib/python3.8/site-packages/sklearn/decomposition/_nmf.py:312: FutureWarning: The &#39;init&#39; value, when &#39;init=None&#39; and n_components is less than n_samples and n_features, will be changed from &#39;nndsvd&#39; to &#39;nndsvda&#39; in 1.1 (renaming of 0.26).
  warnings.warn((&quot;The &#39;init&#39; value, when &#39;init=None&#39; and &quot;
/home/runner/.cache/pypoetry/virtualenvs/mat281-2021-V7B8LTfe-py3.8/lib/python3.8/site-packages/sklearn/decomposition/_nmf.py:1090: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.
  warnings.warn(&quot;Maximum number of iterations %d reached. Increase it to&quot;
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df_algebra_lineal</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span><span class="n">results</span><span class="p">)</span><span class="o">.</span><span class="n">reset_index</span><span class="p">(</span><span class="n">drop</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">df_algebra_lineal</span><span class="p">[</span><span class="s1">&#39;metodo&#39;</span><span class="p">]</span> <span class="o">=</span><span class="n">names</span>
<span class="n">df_algebra_lineal</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>mae</th>
      <th>mse</th>
      <th>rmse</th>
      <th>mape</th>
      <th>maape</th>
      <th>wmape</th>
      <th>mmape</th>
      <th>smape</th>
      <th>metodo</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>2.8050</td>
      <td>12.2999</td>
      <td>3.5071</td>
      <td>0.2869</td>
      <td>0.2169</td>
      <td>0.1460</td>
      <td>0.2183</td>
      <td>0.2140</td>
      <td>PCA</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2.6344</td>
      <td>11.3195</td>
      <td>3.3645</td>
      <td>0.2605</td>
      <td>0.2023</td>
      <td>0.1372</td>
      <td>0.1987</td>
      <td>0.1979</td>
      <td>SVD</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3.5327</td>
      <td>16.4079</td>
      <td>4.0507</td>
      <td>0.4956</td>
      <td>0.2859</td>
      <td>0.1839</td>
      <td>0.3594</td>
      <td>0.2930</td>
      <td>NMF</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p><strong>Enfoque: Manifold Learning</strong></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">modelos_manifold</span><span class="o">=</span> <span class="p">[</span>
    <span class="p">(</span><span class="s1">&#39;Isomap&#39;</span><span class="p">,</span><span class="n">Isomap</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">5</span><span class="p">)),</span>
    <span class="p">(</span><span class="s1">&#39;LocallyLinearEmbedding&#39;</span><span class="p">,</span> <span class="n">LocallyLinearEmbedding</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">5</span><span class="p">)),</span>
    <span class="p">(</span><span class="s1">&#39;MDS&#39;</span><span class="p">,</span>  <span class="n">MDS</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">5</span><span class="p">)),</span>
    <span class="p">(</span><span class="s1">&#39;SpectralEmbedding&#39;</span><span class="p">,</span> <span class="n">SpectralEmbedding</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">5</span><span class="p">)),</span>
    <span class="p">(</span><span class="s1">&#39;TSNE&#39;</span><span class="p">,</span> <span class="n">TSNE</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">)),</span>
<span class="p">]</span>

<span class="n">names</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">modelos_manifold</span><span class="p">]</span>
<span class="n">results</span> <span class="o">=</span> <span class="p">[</span><span class="n">dr_pipeline</span><span class="p">(</span><span class="n">datos</span><span class="p">,</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">modelos_manifold</span><span class="p">]</span>


<span class="n">df_manifold</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span><span class="n">results</span><span class="p">)</span><span class="o">.</span><span class="n">reset_index</span><span class="p">(</span><span class="n">drop</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">df_manifold</span><span class="p">[</span><span class="s1">&#39;metodo&#39;</span><span class="p">]</span> <span class="o">=</span><span class="n">names</span>
<span class="n">df_manifold</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>mae</th>
      <th>mse</th>
      <th>rmse</th>
      <th>mape</th>
      <th>maape</th>
      <th>wmape</th>
      <th>mmape</th>
      <th>smape</th>
      <th>metodo</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>9.4021</td>
      <td>133.7433</td>
      <td>11.5647</td>
      <td>0.9914</td>
      <td>0.5142</td>
      <td>0.4895</td>
      <td>0.7646</td>
      <td>0.5491</td>
      <td>Isomap</td>
    </tr>
    <tr>
      <th>1</th>
      <td>9.1368</td>
      <td>130.7978</td>
      <td>11.4367</td>
      <td>0.8874</td>
      <td>0.5235</td>
      <td>0.4757</td>
      <td>0.7000</td>
      <td>0.5450</td>
      <td>LocallyLinearEmbedding</td>
    </tr>
    <tr>
      <th>2</th>
      <td>8.2218</td>
      <td>112.8162</td>
      <td>10.6215</td>
      <td>0.9734</td>
      <td>0.4603</td>
      <td>0.4280</td>
      <td>0.7028</td>
      <td>0.4823</td>
      <td>MDS</td>
    </tr>
    <tr>
      <th>3</th>
      <td>8.9529</td>
      <td>130.9545</td>
      <td>11.4435</td>
      <td>0.8330</td>
      <td>0.5061</td>
      <td>0.4661</td>
      <td>0.6646</td>
      <td>0.5261</td>
      <td>SpectralEmbedding</td>
    </tr>
    <tr>
      <th>4</th>
      <td>8.8451</td>
      <td>127.1872</td>
      <td>11.2777</td>
      <td>0.9408</td>
      <td>0.5046</td>
      <td>0.4605</td>
      <td>0.7084</td>
      <td>0.5270</td>
      <td>TSNE</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>En este caso en particular, funciona de mejor forma aplicar los m√©todos de descomposici√≥n del <strong>Algebra Lineal</strong> en relaci√≥n de los m√©todos de <strong>Manifold Learning</strong>. La ense√±anza que se lleva de esto que, dependiendo del volumen de datos que se trabaje, la capidad de c√≥mputo y las habilidades de programaci√≥n suficiente, se pueden probar y automatizar varios de estos m√©todos. Por supuesto, quedar√° como responsabilidad del programador buscar el criterio para poder seleccionar el mejor m√©todo (dependiendo del caso en estudio).</p>
</div>
</div>
<div class="section" id="referencia">
<h2>Referencia<a class="headerlink" href="#referencia" title="Permalink to this headline">¬∂</a></h2>
<ol class="simple">
<li><p><a class="reference external" href="https://www.aprendemachinelearning.com/comprende-principal-component-analysis/">In Depth: Principal Component Analysis</a></p></li>
<li><p><a class="reference external" href="https://scikit-learn.org/stable/modules/unsupervised_reduction.html">Unsupervised dimensionality reduction</a></p></li>
</ol>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./mat281/lectures/ml/analisis_no_supervisado"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
        <div class='prev-next-bottom'>
            
    <a class='left-prev' id="prev-link" href="03_clustering.html" title="previous page">Clustering</a>
    <a class='right-next' id="next-link" href="../overfitting/04_overfitting_underfitting.html" title="next page">Overfitting</a>

        </div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Francisco Alfaro Medina<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../../../../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
<script async="" src="https://www.google-analytics.com/analytics.js"></script>
<script>
                        window.ga = window.ga || function () {
                            (ga.q = ga.q || []).push(arguments) };
                        ga.l = +new Date;
                        ga('create', 'UA-177357392-1', 'auto');
                        ga('set', 'anonymizeIp', true);
                        ga('send', 'pageview');
                    </script>

  </body>
</html>